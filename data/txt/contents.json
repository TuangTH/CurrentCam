[{
  "at": 0,
  "title": "",
  "msg": "",
  "msg2": "Current<br> 2020<br> Research project & speculative cinematic proposal by<br>Eli Joteva, Provides Ng, Alexey Yansitov(Ya Nzi)<br> Concieved at Strelka The New Normal, 2019<br><br>Produced with the support of:<br> Strelka Institute, Benjamin Bratton, Metahaven, Liam Young, Nathan Su, Nikita Suslov, Atrem Konevskikh, Michael Villiers, Tuang Thongborisute, Sofia Pia, Mary Anaskina, Elizaveta Dorrer, Nicolay Boyadjiev"
}, {
  "at": 1,
  "title": "Enviromental Reconstructions",
  "msg": "‘Current’ focuses on democratising reconstruction techniques to facilitate a collective contribution to urban archives.",
  "msg2": "‘Current’ focuses on democratising reconstruction techniques to facilitate a collective contribution to urban archives. Instead of using high end t echnologies and softwares that are only available to institutions and corporates, ‘Current’ tested a number of low-end sensors (i.e. mobile phones, Kinect, etc.), open-sourced AI algorithms and photogrammetry frameworks that are readily available to any architects or individuals for environment/event reconstruction. The aim is to formulate a platform that allows individuals to collectively gather and process data. ‘Current’ records the workflow and the final pipeline used to produce the 12-min film prototype."
}, {
  "at": 5,
  "title": "AI OPTIMIZATION",
  "msg": "'Current' configures an aesthetic vocabulary of cinematology, expanding the spectrum of aesthetic semblance by experimenting with image-processing neural networks: Generative Adversarial Neural Network (GAN) and Autoencoder.",
  "msg2": "Evolving from a fantasized persona of otherness in the 70s, Artificial Intelligence has now become a city-scale entity, a distributed network of sensing and processing technologies which complements human intuition and design. The invention of mobile A.I. and cameras allow for participatory authorship in urban design and 3D environment reconstruction from everyday scenarios. This alternative intelligence has the ability to hallucinate and dream in its own definition, forming a collaborative vision between A.I., architects and open-source big data.<br><br>‘Current’ seeks to configure a new aesthetic vocabulary of cinematology, expanding the spectrum of aesthetic semblance by experimenting with image-processing neural networks: Generative Adversarial Neural Network (GAN) and Autoencoder."
}, {
  "at": 8,
  "title": "LIVESTREAM",
  "msg": "Livestream encompasses extraordinary moments alongside an infinite feed of the mundane, it is a communication device that spans across cities and borders to circulate information in real-time through micro-value transactions and branded virtual signalling.",
  "msg2": "Livestream encompasses extraordinary moments alongside an infinite feed of the mundane, it is a communication device that spans across cities and borders to circulate information in real-time through micro-value transactions and branded virtual signalling. Livestream is a distributed technology that gives insights to events and landscapes that are often hidden from institutional broadcasting channels, such as the news media. Its interface is often composed of a main viewing portal of the streamed content, a virtual chat room for viewers to interact, information on all the viewers that are participating, pop up notifications of viewers sending virtual gifts, a loyalty ranking, and a ‘follow’ button. Its qualities give rise to an attention economy that circulates values distinct from traditional moving-image media.<br><br>First, it encompasses compelling moments alongside an infinite feed of the mundane, suggesting a sense of ‘truth’ - an event being preserved in its entirety - to its audience. Second, instead of having to sit in for a standardised amount of time, its quality of mundane and long time span allow its audience the sense of freedom to step in and out of the stream at any moment. Third, it facilitates a participatory authorship, where the interaction between the audience and streamer collaboratively directs, narrates, and curates the experience."
}, {
  "at": 10,
  "title": "Non-Human Perspectives",
  "msg": "The spectrum of content for livestream broadens, embracing perspective from non-human agents, both animal and machine. The infinite data input from autonomous cars forms an open-source cinema, a communication device that is a city scale meta-infrastructure. In what ways can we structure this massive amount of data into a useful tool for urban design and collective thinking? We propose 3D reconstruction of environments and events from  autonomous cars, animal cams, city surveillance cameras and other non-human perspectives."
}, {
  "at": 15,
  "title": "Volumetric Cinema",
  "msg": "The contemporary advent of new sensing technologies has shifted the current experience of visual content to include 3 dimensions. With remote satellite sensing we have transitioned from viewing the earth from a planar map to a 3 dimensional globe. Depth lidar sensors and motion trackers used in cinema, virtual reality and gaming industries have transformed the cinematic language of navigation into an editing technique: instead of pre cut-to-cut montage, we now experience navigation-based world-to-world transitions. Within a VR or gaming environment, the space constructs as the user navigates through it, providing a recursive interaction for users to shape the content they experience in real time through their attention and navigation.",
  "msg2": "The transition from 2 to 3 dimensions has enriched the image with more information.  Currently, all images and videos produced are embedded with spatial metadata. Photogrammetry reconstructions and lidar scans of environments can be localised to GPS-specific locations. When coupled with multiple real-time cameras and sensor inputs of the same location, the informationally rich space of volumetric construction can provide decentralized perspectives to events. Vision mechanisms of self driving cars already use real-time collaborative vision to cross-check what they perceive with each other. Within the framework of volumetric attention-based navigation, Current speculates on the potential of this type of collaborative vision to authenticate truth for users."
}, {
  "at": 26,
  "title": "DEEP FAKES",
  "msg": "The outsourcing of imagination to Artificial intelligence can most readily be observed in the cultural phenomena of generative adversarial networks (GANs) and deep fakes."
}, {
  "at": 30,
  "title": "Microlicencing ",
  "msg": "The emergence of CGI influencers on social media pushes the limits of the body. As non-human entities progressively consume the market’s economy, what is the granularity of a body that can be licensed? What is the criteria for value when crediting animals and plants, and how can we return the resources used up by the attention economy back to nature?"
}]
